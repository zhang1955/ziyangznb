# pyhthon爬虫

### **1、概念：是一种按照一定规则，自动抓取互联网信息的程序或者脚本**

### **2、原理示意图**![查看源图像](../../assets/Untitled/R-C.4f96ffcc800820e14b9ed93562ec34e7)

### 3、基本流程

- 获取数据：通过HTTP库向目标点发起请求，请求包括额外的header等信息，如火服务器能正常响应，会得到一个Request，便是所要获取的页面信息。

- 解析内容：得到的内容可能是HTML、json等格式，可以永页面解析库、正则表达式等进行解析。
- 保存数据：保存的形式多样，可以保存为文本，也可以保存在数据库。

### 4、引入模块

```python
from bs4 import BeautifulSoup  # 页面解析，获取数据
import re  # 进行文字匹配
import urllib.request,urllib.error  # 制定url，获取网页数据
import xlwt  # 进行excel操作
import sqlite3  #进行SQL数据库操作
```



### 5、简单的爬取数据

```python
url = "地址"
headers = {  # 模拟浏览器头部信息，向服务器发送信息，伪装用的
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36"
}  # 用户代理：高速服务器我们是什么样的机器(本质告诉浏览器，我们可以接收什么水平的信息)

req = urllib.request.Request(url=url, headers=headers)  # 携带地址和头部信息发送请求

response = urllib.request.urlopen(req) # 接收反馈信息

print(response.read().decode("utf-8"))
```

### 6、解析信息

```python
from bs4 import BeautifulSoup

file = open ("douban.html","rb")
html = file.read().decode("utf-8")
bs = BeautifulSoup(html,"html.parser")

# print(bs.title)
# 只能找到对应文件第一个此标签的内容和标签本身
# 后面再加上.string  只拿标签里的内容
# 后面加上.attars  可以拿到一个标签里的所有属性

# print(bs.head.contents[1])  # 用列表索引获取它的第一元素
for child in bs.head.contents:
    print(child)  # 获取它的所有子节点，返回一个生成器
```

### 7、正则表达式：一些常用的

![在这里插入图片描述](../../assets/爬虫/20200408162201990.png)











